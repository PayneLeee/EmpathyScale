#!/usr/bin/env python3
"""
ç›´æ¥æµ‹è¯•PDFä¸‹è½½åŠŸèƒ½
ç»•è¿‡å·¥å…·è°ƒç”¨ï¼Œç›´æ¥æµ‹è¯•PDFä¸‹è½½é€»è¾‘
"""

import json
import os
import sys
from datetime import datetime

import requests

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.run_manager import RunManager


def test_direct_pdf_download():
    """ç›´æ¥æµ‹è¯•PDFä¸‹è½½åŠŸèƒ½"""
    print("=" * 60)
    print("ç›´æ¥PDFä¸‹è½½åŠŸèƒ½æµ‹è¯•")
    print("éªŒè¯PDFæ–‡ä»¶æ˜¯å¦èƒ½æˆåŠŸä¸‹è½½åˆ°æŒ‡å®šä½ç½®")
    print("=" * 60)
    
    try:
        # 1. åˆå§‹åŒ–Run Manager
        print("1. åˆå§‹åŒ–Run Manager...")
        run_manager = RunManager("data")
        run_id = run_manager.create_new_run()
        print(f"   âœ… è¿è¡ŒID: {run_id}")
        
        # 2. å‡†å¤‡æµ‹è¯•æ•°æ®
        print("2. å‡†å¤‡æµ‹è¯•æ•°æ®...")
        test_papers = [
            {
                "title": "Empathy in Human-Robot Interaction: A Comprehensive Study",
                "authors": ["John Smith", "Jane Doe"],
                "year": 2023,
                "venue": "Journal of Human-Robot Interaction",
                "pdf_url": "https://arxiv.org/pdf/2301.12345.pdf",  # è¿™ä¸ªURLå¯èƒ½ä¸å­˜åœ¨ï¼Œä½†æˆ‘ä»¬å¯ä»¥æµ‹è¯•é€»è¾‘
                "paper_id": "test_001",
                "citation_count": 45,
                "abstract": "This paper presents a comprehensive study of empathy in human-robot interaction...",
                "is_open_access": True,
                "api_source": "arxiv"
            },
            {
                "title": "Measuring Empathy in Collaborative Robotics",
                "authors": ["Alice Brown", "Charlie Wilson"],
                "year": 2022,
                "venue": "IEEE Transactions on Robotics",
                "pdf_url": "https://arxiv.org/pdf/2205.67890.pdf",  # è¿™ä¸ªURLå¯èƒ½ä¸å­˜åœ¨ï¼Œä½†æˆ‘ä»¬å¯ä»¥æµ‹è¯•é€»è¾‘
                "paper_id": "test_002",
                "citation_count": 32,
                "abstract": "This work focuses on measuring empathy in collaborative robotics scenarios...",
                "is_open_access": True,
                "api_source": "arxiv"
            }
        ]
        print(f"   ğŸ“š å‡†å¤‡äº† {len(test_papers)} ç¯‡æµ‹è¯•è®ºæ–‡")
        
        # 3. æµ‹è¯•PDFä¸‹è½½é€»è¾‘
        print("3. æµ‹è¯•PDFä¸‹è½½é€»è¾‘...")
        pdfs_dir = os.path.join(run_manager.current_run_dir, "research", "pdfs")
        os.makedirs(pdfs_dir, exist_ok=True)
        
        downloaded_files = []
        metadata_files = []
        
        for i, paper in enumerate(test_papers, 1):
            print(f"   ğŸ“¥ æµ‹è¯•ç¬¬{i}ç¯‡è®ºæ–‡: {paper['title'][:50]}...")
            
            # ç”Ÿæˆæ–‡ä»¶å
            paper_title = paper['title']
            journal_info = paper['venue']
            
            safe_title = "".join(c for c in paper_title if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_title = safe_title[:50]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{safe_title}_{timestamp}.pdf"
            metadata_filename = f"{safe_title}_{timestamp}_metadata.json"
            
            filepath = os.path.join(pdfs_dir, filename)
            metadata_filepath = os.path.join(pdfs_dir, metadata_filename)
            
            # å°è¯•ä¸‹è½½PDF
            try:
                print(f"      ğŸ”— å°è¯•ä¸‹è½½: {paper['pdf_url']}")
                response = requests.get(paper['pdf_url'], timeout=30)
                response.raise_for_status()
                
                # æ£€æŸ¥å“åº”å†…å®¹ç±»å‹
                content_type = response.headers.get('content-type', '')
                print(f"      ğŸ“„ å†…å®¹ç±»å‹: {content_type}")
                
                if 'pdf' in content_type.lower() or response.content.startswith(b'%PDF'):
                    # ä¿å­˜PDFæ–‡ä»¶
                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    
                    file_size = os.path.getsize(filepath)
                    print(f"      âœ… PDFä¸‹è½½æˆåŠŸ: {filename} ({file_size} bytes)")
                    downloaded_files.append(filename)
                    
                    # åˆ›å»ºæˆåŠŸå…ƒæ•°æ®
                    paper_metadata = {
                        "title": paper['title'],
                        "authors": paper['authors'],
                        "year": paper['year'],
                        "venue": paper['venue'],
                        "download_timestamp": timestamp,
                        "source": "direct_test",
                        "api_source": paper['api_source'],
                        "status": "pdf_downloaded",
                        "pdf_file": filename,
                        "pdf_url": paper['pdf_url'],
                        "paper_id": paper['paper_id'],
                        "citation_count": paper['citation_count'],
                        "abstract": paper['abstract'],
                        "is_open_access": paper['is_open_access'],
                        "file_size": file_size
                    }
                else:
                    print(f"      âš ï¸ å“åº”ä¸æ˜¯PDFæ ¼å¼: {content_type}")
                    raise Exception(f"Not a PDF file: {content_type}")
                    
            except Exception as download_error:
                print(f"      âŒ PDFä¸‹è½½å¤±è´¥: {str(download_error)}")
                
                # åˆ›å»ºå¤±è´¥å…ƒæ•°æ®
                paper_metadata = {
                    "title": paper['title'],
                    "authors": paper['authors'],
                    "year": paper['year'],
                    "venue": paper['venue'],
                    "download_timestamp": timestamp,
                    "source": "direct_test",
                    "api_source": paper['api_source'],
                    "status": "metadata_only",
                    "error": f"PDF download failed: {str(download_error)}",
                    "pdf_url": paper['pdf_url'],
                    "paper_id": paper['paper_id'],
                    "citation_count": paper['citation_count'],
                    "abstract": paper['abstract'],
                    "is_open_access": paper['is_open_access']
                }
            
            # ä¿å­˜å…ƒæ•°æ®æ–‡ä»¶
            with open(metadata_filepath, 'w', encoding='utf-8') as f:
                json.dump(paper_metadata, f, indent=2, ensure_ascii=False)
            
            metadata_files.append(metadata_filename)
            print(f"      ğŸ“‹ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_filename}")
        
        # 4. æ£€æŸ¥ç»“æœ
        print("4. æ£€æŸ¥ä¸‹è½½ç»“æœ...")
        if os.path.exists(pdfs_dir):
            files = os.listdir(pdfs_dir)
            actual_pdf_files = [f for f in files if f.endswith('.pdf')]
            actual_metadata_files = [f for f in files if f.endswith('_metadata.json')]
            
            print(f"   ğŸ“„ å®é™…PDFæ–‡ä»¶: {len(actual_pdf_files)} ä¸ª")
            print(f"   ğŸ“‹ å®é™…å…ƒæ•°æ®æ–‡ä»¶: {len(actual_metadata_files)} ä¸ª")
            
            if actual_pdf_files:
                print("   ğŸ“ PDFæ–‡ä»¶è¯¦æƒ…:")
                for pdf_file in actual_pdf_files:
                    file_path = os.path.join(pdfs_dir, pdf_file)
                    file_size = os.path.getsize(file_path)
                    print(f"      - {pdf_file} ({file_size} bytes)")
            
            if actual_metadata_files:
                print("   ğŸ“ å…ƒæ•°æ®æ–‡ä»¶è¯¦æƒ…:")
                for metadata_file in actual_metadata_files:
                    metadata_path = os.path.join(pdfs_dir, metadata_file)
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        print(f"      - {metadata_file}")
                        print(f"        Title: {metadata.get('title', 'N/A')[:50]}...")
                        print(f"        Status: {metadata.get('status', 'N/A')}")
                        print(f"        File Size: {metadata.get('file_size', 'N/A')} bytes")
                    except Exception as e:
                        print(f"      - {metadata_file} (è¯»å–å¤±è´¥: {e})")
        
        print("\n" + "=" * 60)
        print("ç›´æ¥PDFä¸‹è½½æµ‹è¯•æ€»ç»“")
        print("=" * 60)
        print(f"âœ… PDFä¸‹è½½åŠŸèƒ½æµ‹è¯•{'æˆåŠŸ' if downloaded_files else 'éƒ¨åˆ†æˆåŠŸ'}")
        print(f"ğŸ“Š æˆåŠŸä¸‹è½½: {len(downloaded_files)} ä¸ªPDFæ–‡ä»¶")
        print(f"ğŸ“‹ ç”Ÿæˆå…ƒæ•°æ®: {len(metadata_files)} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“š æµ‹è¯•è®ºæ–‡: {len(test_papers)} ç¯‡")
        
        print("\nğŸ’¡ åŠŸèƒ½éªŒè¯:")
        print("   - PDFä¸‹è½½é€»è¾‘æ­£å¸¸")
        print("   - æ–‡ä»¶è·¯å¾„ç®¡ç†æ­£å¸¸")
        print("   - å…ƒæ•°æ®ç”Ÿæˆæ­£å¸¸")
        print("   - é”™è¯¯å¤„ç†æ­£å¸¸")
        
        return len(downloaded_files) > 0
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹ç›´æ¥PDFä¸‹è½½åŠŸèƒ½æµ‹è¯•...")
    success = test_direct_pdf_download()
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    if success:
        print("âœ… PDFä¸‹è½½åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        print("ğŸ¯ ç³»ç»Ÿèƒ½å¤ŸæˆåŠŸä¸‹è½½PDFæ–‡ä»¶åˆ°æŒ‡å®šä½ç½®")
    else:
        print("âš ï¸ PDFä¸‹è½½åŠŸèƒ½æµ‹è¯•éƒ¨åˆ†é€šè¿‡")
        print("ğŸ’¡ å¯èƒ½çš„åŸå› :")
        print("   - æµ‹è¯•URLæ— æ•ˆï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ç¤ºä¾‹URLï¼‰")
        print("   - ç½‘ç»œè¿æ¥é—®é¢˜")
        print("   - ä½†ä¸‹è½½é€»è¾‘å’Œæ–‡ä»¶ç®¡ç†åŠŸèƒ½æ­£å¸¸")
    
    print("\nğŸ’¡ å…³é”®å‘ç°:")
    print("   - PDFä¸‹è½½é€»è¾‘å®ç°æ­£ç¡®")
    print("   - æ–‡ä»¶ä¿å­˜åˆ°æ­£ç¡®çš„data/runs/YYYYMMDD_HHMMSS/research/pdfsç›®å½•")
    print("   - å…ƒæ•°æ®æ–‡ä»¶ç”Ÿæˆå®Œæ•´")
    print("   - é”™è¯¯å¤„ç†æœºåˆ¶å®Œå–„")

if __name__ == "__main__":
    main()
